# AUTOGENERATED! DO NOT EDIT! File to edit: ../../nbs/00_extract.ipynb.

# %% ../../nbs/00_extract.ipynb 2
from __future__ import annotations
import relax
from relax.utils import *
from relax.methods import *
from relax import DataModule
from relax.import_essentials import *
from relax.evaluate import compute_proximity, compute_validity
from relax.ml_model import MLModule
from relax.explain import Explanation, prepare_cf_module
from relax.data_module import DEFAULT_DATA
import einops
from einops import rearrange, reduce, repeat
import copy

# %% auto 0
__all__ = ['RetrainData', 'agreement', 'partition_data', 'AttackFn', 'NaturalTraining', 'QueryModelExtraction',
           'ModelExtractionCF', 'DualCFAttack', 'dual_cfs_attack', 'query_model', 'visualize_model_xs_cfs',
           'get_gen_fn']

# %% ../../nbs/00_extract.ipynb 3
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import pandas as pd
from sklearn.utils import shuffle
from sklearn.base import BaseEstimator

# %% ../../nbs/00_extract.ipynb 5
class RetrainData:
    """Wrapper dataset for retraining dual counterfactuals."""

    def __init__(
        self,
        cfs: Array, 
        dual_cfs: Array,
        pred_fn: Callable[[Array], Array],
        name: str = "CFModule",
    ):
        # (n, k) -> (n, 1, k) or (n, b, k)
        cfs, dual_cfs = map(lambda x: rearrange(x, "n ... k -> n (...) k"), [cfs, dual_cfs])
        cfs_ys = pred_fn(cfs).argmax(axis=-1) # (n, 1)
        dual_cfs_ys = pred_fn(dual_cfs).argmax(axis=-1) # (n, b)

        self.parse_dual_cfs = einops.parse_shape(dual_cfs, "n ... k")
        
        retrain_xs = jnp.concatenate([cfs, dual_cfs], axis=1) # (n, b+1, k)
        retrain_ys = jnp.concatenate([cfs_ys, dual_cfs_ys], axis=1) # (n, b+1)
        # self.dual_cfs_benchmarks = self._evaluate_dual_cfs(cfs, dual_cfs, pred_fn, name)
        self.retrain_xs, self.retrain_ys = shuffle(retrain_xs, retrain_ys)
        self.key = keras.random.SeedGenerator(get_config().global_seed)
        self.pred_fn = pred_fn
        self.name = name

    def evaluate(self, pred_fn=None):
        if pred_fn is None: pred_fn = self.pred_fn
        cfs, dual_cfs = self.retrain_xs[..., 0, :], self.retrain_xs[..., 1:, :]
        validity = compute_validity(cfs, dual_cfs, pred_fn)
        proximity = compute_proximity(cfs, dual_cfs)
        return pd.DataFrame(
            {
                "validity": validity,
                "proximity": proximity,
            },
            index=[self.name],
        )

    def __len__(self):
        return len(self.retrain_xs)

    def __getitem__(self, idx):
        return self.retrain_xs[idx], self.retrain_ys[idx]

    def sample(self, n: int) -> Tuple[Array, Array]: # (n * (b+1), k), (n * (b+1), 1)
        key, _ = jrand.split(self.key.next())
        indices = jnp.arange(self.retrain_xs.shape[0])
        indices = jrand.shuffle(key, indices)[:n]
        sampled_xs, sampled_ys = self.retrain_xs[indices], self.retrain_ys[indices]
        return (
            rearrange(sampled_xs, "n ... k -> (n ...) k"), 
            rearrange(sampled_ys, "n ... -> (n ...) 1")
        )


# %% ../../nbs/00_extract.ipynb 6
def _train_relax_model(
    model: MLModule, # Model to mimic and perform extraction attack
    dataset: tuple[Array, Array], # Dataset to train on
    train_kwargs: dict = {}, # Keyword arguments for training
):
    default_train_kwargs = {
        'epochs': 100, 'validation_split': 0., 'batch_size': 32, 'verbose': False
    }
    default_train_kwargs.update(train_kwargs or {})
    model.train(dataset, **default_train_kwargs)
    return model

# %% ../../nbs/00_extract.ipynb 7
def _train_sklearn_model(
    model: BaseEstimator, # Model to mimic and perform extraction attack
    dataset: tuple[Array, Array], # Dataset to train on
    **kwargs
):
    model.fit(*dataset)
    return model

# %% ../../nbs/00_extract.ipynb 8
def _train_model(
    model: MLModule | BaseEstimator, # Model to mimic and perform extraction attack
    dataset: tuple[Array, Array], # Dataset to train on
    train_kwargs: dict = {}, # Keyword arguments for training
) -> MLModule | BaseEstimator:
    """Trains a model to perform an extraction attack by mimicking the provided model on the given dataset."""
    
    if isinstance(model, MLModule):
        return _train_relax_model(model, dataset, train_kwargs)
    elif isinstance(model, BaseEstimator):
        return _train_sklearn_model(model, dataset, **train_kwargs)
    else:
        raise ValueError(f"Unsupported model type: {type(model).__name__}")

# %% ../../nbs/00_extract.ipynb 10
def _model_pred_fn_factory(model: MLModule | BaseEstimator) -> Callable[[Array], Array]:
    if isinstance(model, MLModule):
        return model.pred_fn
    elif isinstance(model, BaseEstimator):
        return model.predict_proba
    else:
        raise ValueError(f"Unsupported model type: {type(model).__name__}")

def agreement(
    exp: Explanation, # Explanation object which contains the true model
    model: MLModule | BaseEstimator, # Model to mimic and perform extraction attack
    test_xs: Array, # Test set
):
    """Computes the agreement between the true and the extracted model on the test set."""

    extracted_model = _model_pred_fn_factory(model)
    out = extracted_model(test_xs).argmax(axis=1)
    true_out = exp.pred_fn(test_xs).argmax(axis=1)
    agree = (out == true_out).mean()
    return agree

# %% ../../nbs/00_extract.ipynb 11
def partition_data(
    val_xs: Array, # Validation set
    val_ys: Array, # Validation set labels
    val_cfs: Array, # Validation set counterfactuals
    test_xs: Array = None
):
    if test_xs is None:
        val_xs, test_xs, val_ys, test_ys, val_cfs, test_cfs = train_test_split(
            val_xs, val_ys, val_cfs, test_size=0.5, random_state=0
        )
    return val_xs, val_ys, val_cfs, test_xs

# %% ../../nbs/00_extract.ipynb 12
class AttackFn:
    """Base class for extraction attacks."""

    def __init__(
        self, 
        attack_model: MLModule | BaseEstimator, # Model to mimic and perform extraction attack
        **kwargs
    ):
        self.model = attack_model
        self.global_key = jrand.PRNGKey(get_config().global_seed)
    
    @abstractmethod
    def __call__(
        self,
        exp: Explanation, # Generated CF explanations
        n_queries: int = 100, # Number of queries to use for extracting model
        rng_key: Array = None, # Random number generator key
        train_kwargs: dict = None, # Keyword arguments for retraining
        shuffle: bool = False, # Shuffle the dataset before retraining
    ):
        raise NotImplementedError

# %% ../../nbs/00_extract.ipynb 13
class NaturalTraining(AttackFn):
    """Use (x_test, F(x_test)) to extract the model."""

    def __call__(
        self,
        exp: Explanation, # Generated CF explanations
        n_queries: int = 100, # Number of queries to use for extracting model
        rng_key: Array = None, # Random number generator key
        train_kwargs: dict = None, # Keyword arguments for retraining
        shuffle: bool = True, # Shuffle the dataset before retraining
    ):
        exp_val = exp['val']
        val_xs = exp_val['xs']
        val_ys = exp_val['ys']
        rng_key = self.global_key if rng_key is None else rng_key
        
        # get n_queries and their probability predictions
        if shuffle:
            self.queried_idx = jrand.permutation(
                rng_key, jnp.arange(len(val_xs)), independent=True)[:n_queries]
        else:
            self.queried_idx = jnp.arange(n_queries)
        queried_xs = val_xs[self.queried_idx]
        queried_ys = jax.nn.one_hot(
            val_ys[self.queried_idx].reshape(-1), num_classes=2)
        
        print(f"Extracting model using {n_queries} queries.")
        print(f"queried_xs.shape: {queried_xs.shape}; queried_ys.shape: {queried_ys.shape}")
        model = _train_model(self.model, (queried_xs, queried_ys), train_kwargs)
        return model

# %% ../../nbs/00_extract.ipynb 14
class QueryModelExtraction(AttackFn):
    """Use (x_test, F(x_test)) to extract the model."""

    def __call__(
        self,
        exp: Explanation, # Generated CF explanations
        n_queries: int = 100, # Number of queries to use for extracting model
        rng_key: Array = None, # Random number generator key
        train_kwargs: dict = None, # Keyword arguments for retraining
        shuffle: bool = True, # Shuffle the dataset before retraining
    ):
        exp_val = exp['val']
        val_xs = exp_val['xs']
        rng_key = self.global_key if rng_key is None else rng_key
        
        # get n_queries and their probability predictions
        if shuffle:
            self.queried_idx = jrand.permutation(
                rng_key, jnp.arange(len(val_xs)), independent=True)[:n_queries]
        else:
            self.queried_idx = jnp.arange(n_queries)
        queried_xs = val_xs[self.queried_idx]
        queried_ys = exp.pred_fn(queried_xs)
        
        print(f"Extracting model using {n_queries} queries.")
        print(f"queried_xs.shape: {queried_xs.shape}; queried_ys.shape: {queried_ys.shape}")
        model = _train_model(self.model, (queried_xs, queried_ys), train_kwargs)
        return model

# %% ../../nbs/00_extract.ipynb 15
class ModelExtractionCF(AttackFn):
    """Use (x_test, F(x_test)) and (cf, F(cf)) to extract the model."""

    def __call__(
        self,
        exp: Explanation, # Generated CF explanations
        n_queries: int = 100, # Number of queries to use for extracting model
        rng_key: Array = None, # Random number generator key
        train_kwargs: dict = None, # Keyword arguments for retraining
        shuffle: bool = True, # Shuffle the dataset before retraining
    ):
        exp_val = exp['val']
        val_xs, val_cfs = exp_val['xs'], exp_val['cfs']
        rng_key = self.global_key if rng_key is None else rng_key

        # get n_queries and their probability predictions
        if shuffle:
            self.queried_idx = jrand.permutation(
                rng_key, jnp.arange(len(val_xs)), independent=True)[:n_queries]
        else:
            self.queried_idx = jnp.arange(n_queries)
        queried_xs = val_xs[self.queried_idx]
        queried_ys = exp.pred_fn(queried_xs)
        # get their counterfactuals
        queried_cfs = einops.rearrange(
            val_cfs[self.queried_idx], 'n b k -> (n b) k'
        ) 
        queried_cfs_ys = exp.pred_fn(queried_cfs)

        # retrain model
        retrain_xs = jnp.concatenate([queried_xs, queried_cfs], axis=0)
        retrain_ys = jnp.concatenate([queried_ys, queried_cfs_ys], axis=0)
        
        print(f"Extracting model using {n_queries} queries.")
        print(f"retrain_xs.shape: {retrain_xs.shape}; retrain_ys.shape: {retrain_ys.shape}")
        
        model = _train_model(self.model, (retrain_xs, retrain_ys), train_kwargs)
        return model

# %% ../../nbs/00_extract.ipynb 16
class DualCFAttack(ModelExtractionCF):
    """Use (cf_test, F(cf_test)) and (cf_cf, F(cf_cf)) to extract the model."""

    def __init__(
        self, 
        attack_model: MLModule | BaseEstimator, 
        cf_generate_fn: Callable[[Explanation], Explanation] = None, # perform dual cf generation
        **kwargs
    ):
        super().__init__(attack_model, **kwargs)
        self.cf_generate_fn = cf_generate_fn

    def set_cf_generate_fn(self, cf_generate_fn: Callable[[Explanation], Explanation]):
        self.cf_generate_fn = cf_generate_fn

    def __call__(
        self,
        exp: Explanation, # Generated CF explanations
        n_queries: int = 100, # Number of queries to use for extracting model
        rng_key: Array = None, # Random number generator key
        train_kwargs: dict = None, # Keyword arguments for retraining
        shuffle: bool = True, # Shuffle the dataset before retraining
    ):
        # generate dual cfs
        exp = self.cf_generate_fn(exp)
        
        exp_val = exp['val']
        val_cfs, val_cf_cfs = exp_val['xs'], exp_val['cfs']
        rng_key = self.global_key if rng_key is None else rng_key

        # get n_queries and their probability predictions
        if shuffle:
            self.queried_idx = jrand.permutation(
                rng_key, jnp.arange(len(val_cfs)), independent=True)[:n_queries]
        else:
            self.queried_idx = jnp.arange(n_queries)
        queried_cfs = val_cfs[self.queried_idx] # (n, k)
        queried_cfs_ys = exp.pred_fn(queried_cfs)
        # get dual cfs and their probability predictions
        queried_cf_cfs = einops.rearrange(
            val_cf_cfs[self.queried_idx], 'n b k -> (n b) k'
        )
        queried_cf_cfs_ys = exp.pred_fn(queried_cf_cfs)

        # retrain model
        retrain_xs = jnp.concatenate([queried_cf_cfs, queried_cfs], axis=0)
        retrain_ys = jnp.concatenate([queried_cf_cfs_ys, queried_cfs_ys], axis=0)
        
        print(f"Extracting model using {n_queries} queries.")
        print(f"retrain_xs.shape: {retrain_xs.shape}; retrain_ys.shape: {retrain_ys.shape}")
        
        model = _train_model(self.model, (retrain_xs, retrain_ys), train_kwargs)
        
        self.dual_exp = exp
        return model

# %% ../../nbs/00_extract.ipynb 17
def dual_cfs_attack(
    exp: Explanation, # Generated CF explanations
    cf_module: CFModule, # CF module used to generate CFs
    model: MLModule | BaseEstimator, # Model to mimic and perform extraction attack
    test_xs: Array = None, # Test data to evaluate the model
    n_queries: int = 10, # Number of queries to use for retraining
    rng_key: Array = None, # Random number generator keys
    train_kwargs: dict = None, # Keyword arguments for retraining
    verbose: bool = False, 
    return_dual_cfs: bool = False,
):
    """Implements the dual CF attack.
    link: https://dl.acm.org/doi/abs/10.1145/3531146.3533188
    """
    
    exp_val = exp['val']
    val_xs, val_ys, val_cfs, test_xs = partition_data(
        exp_val['xs'], exp_val['ys'], exp_val['cfs'], test_xs
    )
    
    # Generate dual CFs
    val_cfs = einops.rearrange(val_cfs, 'n b k -> (n b) k')
    generate_cf_fn_partial = ft.partial(cf_module.generate_cf, pred_fn=exp.pred_fn)
    rng_key = jrand.PRNGKey(get_config().global_seed) if rng_key is None else rng_key
    dual_cfs = jax.vmap(generate_cf_fn_partial)(val_cfs, rng_key=jrand.split(rng_key, val_cfs.shape[0]))
    # Create retraining dataset
    rt_ds = RetrainData(val_cfs, dual_cfs, exp.pred_fn)
    # Retrain model
    model = _train_model(model, rt_ds.sample(n_queries), train_kwargs)
    # Calculate agreement
    agree = agreement(exp, model, test_xs)

    if verbose:
        print(f"train: {exp['train']['xs'].shape}")
        print(f"val: {val_xs.shape}")
        print(f"test: {test_xs.shape}")
        print("dual_cf benchmark: \n",rt_ds.evaluate())

    if return_dual_cfs:
        return agree, model, dual_cfs
    return agree, model

# %% ../../nbs/00_extract.ipynb 18
def query_model(
    exp: Explanation, # Generated CF explanations
    cf_module: CFModule, # CF module used to generate CFs
    model: MLModule | BaseEstimator, # Model to mimic and perform extraction attack
    test_xs: Array = None, # Test data to evaluate the model
    n_queries: int = 10, # Number of queries to use for retraining
    rng_key: Array = None, # Random number generator keys
    train_kwargs: dict = None, # Keyword arguments for retraining
    verbose: bool = False, 
    return_dual_cfs: bool = False, # Ignored
):
    exp_val = exp['val']
    val_xs, val_ys, val_cfs, test_xs = partition_data(
        exp_val['xs'], exp_val['ys'], exp_val['cfs'], test_xs
    )

    rng_key = jrand.PRNGKey(get_config().global_seed) if rng_key is None else rng_key
    queried_xs = jrand.shuffle(rng_key, val_xs)[:n_queries, :]
    queried_ys = exp.pred_fn(queried_xs).argmax(axis=1).reshape(-1, 1)
    
    model = _train_model(model, (queried_xs, queried_ys), train_kwargs)
    # Calculate agreement
    agree = agreement(exp, model, test_xs)
    return agree, model

# %% ../../nbs/00_extract.ipynb 19
def visualize_model_xs_cfs(xs, cfs, model, model_1=None):
    """Visualizes the decision boundary of the model and the counterfactuals."""
    # Plot original data points
    plt.scatter(xs[:, 0], xs[:, 1], c='blue', label='Data Points')
    
    # Plot counterfactuals
    plt.scatter(cfs[:, 0], cfs[:, 1], c='red', label='Counterfactuals')
    # for x, cf in zip(xs, cfs):
    #     plt.arrow(x[0], x[1], cf[0] - x[0], cf[1] - x[1], head_width=0.1, head_length=0.2, fc='black', ec='black')
    
    # Generate grid for decision boundary
    x_min, x_max = xs[:, 0].min() - 0.1, xs[:, 0].max() + 0.1
    y_min, y_max = xs[:, 1].min() - 0.1, xs[:, 1].max() + 0.1
    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 1000), np.linspace(y_min, y_max, 1000))
    grid_points = np.c_[xx.ravel(), yy.ravel()]
    
    # Plot decision boundary for model
    Z = model(grid_points)[:, 1]  
    Z = Z.reshape(xx.shape)
    plt.contourf(xx, yy, Z, levels=[0., 0.5, 1.0], alpha=0.3, colors=['blue', 'white', 'yellow'])
     
    # Plot decision boundary for model_1 if provided
    if model_1:
        Z1 = model_1(grid_points)
        Z1 = Z1.reshape(xx.shape)
        plt.contour(xx, yy, Z1, levels=[0.5], colors='green', linestyles='dashed')
    
    plt.legend()
    plt.show()


# %% ../../nbs/00_extract.ipynb 25
def get_gen_fn(cf_module: CFModule):
    def generate_cf_fn(exp: Explanation) -> Explanation:
        cfs = einops.rearrange(exp.cfs, 'n b k -> (n b) k')
        generate_cf_fn_partial = ft.partial(cf_module.generate_cf, pred_fn=exp.pred_fn)
        rng_key = jrand.PRNGKey(get_config().global_seed)
        dual_cfs = jax.vmap(generate_cf_fn_partial)(cfs, rng_key=jrand.split(rng_key, cfs.shape[0]))
        
        print(f"cfs.shape: {cfs.shape}; dual_cfs.shape: {dual_cfs.shape}")
        return Explanation(
            cfs=dual_cfs,
            pred_fn=exp.pred_fn,
            xs=cfs,
            ys=1-exp.ys,
            cf_name="dual_cf"
        )
    return generate_cf_fn

