# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_watermarker.ipynb.

# %% ../nbs/02_watermarker.ipynb 2
from __future__ import annotations
import relax
from relax.utils import *
from relax.methods import *
from relax import DataModule
from relax.import_essentials import *
from relax.evaluate import compute_proximity, compute_validity
from relax.ml_model import MLModule
from relax.explain import Explanation, prepare_cf_module
from relax.data_module import DEFAULT_DATA
from relax.methods.l2c import *
import einops
from einops import rearrange, reduce, repeat
from keras.random import SeedGenerator
import copy
import keras.losses as L
from jax_tqdm import scan_tqdm
import jax_dataloader as jdl
import haiku as hk
from flax.struct import dataclass
from keras.utils import Progbar
from keras.src.utils.io_utils import print_msg
from jax_dataloader.utils import asnumpy
# jax.devices()

# %% auto 0
__all__ = ['PredictiveModel', 'scaled_sigmoid', 'Watermarker', 'watermark_step', 'WatermarkerTrainer']

# %% ../nbs/02_watermarker.ipynb 3
from .trainer import *
from .watermark import hard_kl_divergence, keras2haiku
from relax.legacy.utils import make_hk_module
from sklearn.model_selection import train_test_split
from sklearn.datasets import make_classification


# %% ../nbs/02_watermarker.ipynb 4
class PredictiveModel(hk.Module):
    """A basic predictive model for binary classification."""
    
    def __init__(
        self,
        sizes: List[int], # Sequence of layer sizes.
        dropout_rate: float = 0.3,  # Dropout rate.
        name: Optional[str] = None,  # Name of the module.
    ):
        """A basic predictive model for binary classification."""
        super().__init__(name=name)
        self.sizes = sizes
        self.dropout_rate = dropout_rate

    def __call__(self, x: jnp.ndarray, is_training: bool = True):
        w_init = hk.initializers.VarianceScaling(2.0, "fan_in", "uniform")
        leaky_relu = ft.partial(jax.nn.leaky_relu, negative_slope=0.2)
        dropout_rate = self.dropout_rate if is_training else 0.0
        
        x = hk.nets.MLP(
            self.sizes, 
            w_init=w_init,
            activation=leaky_relu,
        )(x, dropout_rate=dropout_rate, rng=hk.next_rng_key())
        x = jax.nn.softmax(x)
        return x

# %% ../nbs/02_watermarker.ipynb 5
def scaled_sigmoid(x):
    return 2 * jax.nn.sigmoid(x) - 1

# %% ../nbs/02_watermarker.ipynb 6
class Watermarker(hk.Module):
    def __init__(
        self, 
        layers: list[int],
        proj_fn: Callable[[jnp.ndarray], jnp.ndarray] = scaled_sigmoid, # TODO: eps
        dropout_rate: float = 0.3,
        name: str | None = None
    ):
        super().__init__(name)
        w_init = hk.initializers.VarianceScaling(2.0, "fan_in", "uniform")
        leaky_relu = ft.partial(jax.nn.leaky_relu, negative_slope=0.2)
        
        self.wm = hk.nets.MLP(
            layers[:-1], w_init=w_init, activation=leaky_relu, name="watermarker"
        )
        self.linear = hk.Linear(layers[-1], w_init=w_init, name="linear")
        self.dropout_rate = dropout_rate
        self.proj_fn = proj_fn

    def __call__(self, x: jnp.ndarray, is_training: bool = True):
        # key_g, key_s, key_ber = jrand.split(hk.next_rng_key(), 3)
        dropout_rate = self.dropout_rate if is_training else 0.0
        # The sector decides which features to keep
        delta = self.wm(x, dropout_rate=dropout_rate, rng=hk.next_rng_key())
        delta = self.linear(delta)
        delta = self.proj_fn(delta)
        out = x + delta
        return out

# %% ../nbs/02_watermarker.ipynb 8
class Watermarker(hk.Module):

    def __init__(self, w_init=None, b_init=None):
        super().__init__()
        self.w_init = w_init or jnp.ones
        self.b_init = b_init or jnp.zeros
    
    def __call__(self, x: Array, is_training: bool = True):
        k = x.shape[-1]
        w = hk.get_parameter("w", shape=[k], dtype=x.dtype, init=self.w_init)
        b = hk.get_parameter("b", shape=[k], dtype=x.dtype, init=self.b_init)
        return jnp.einsum("ij,j->ij", x, w) + b

# %% ../nbs/02_watermarker.ipynb 10
@ft.partial(jax.jit, static_argnums=(2,3,4,5,6,7))
def watermark_step(
    batch: Tuple[jax.Array, ...],
    state: TrainState,
    models: Tuple[hk.Transformed, hk.Transformed], # pred_model, watermarker
    optimizers: Tuple[optax.GradientTransformation, ...], # pred_opt, wm_opt
    steps: int = 1, # outer steps
    k: int = 1, # inner steps
    proj_fn: Callable[[jnp.ndarray], jnp.ndarray] = None, # projection function for wm params
    apply_constraints_fn: Callable = None,
) -> TrainState:
    # state:
    # - params: (w1, w2), old_w, wm_params
    # - opt_states: (w1_state, w2_state), wm_state
    # - rng_key
       
    def forward(weights, key, xs, is_training):
        return jax.vmap(pred_model.apply, in_axes=(0, None, None, None))(
            weights, key, xs, is_training
        )
    
    def vapply_constraints(cfs, hard, rng_key):
        cfs = einops.rearrange(cfs, '(B C) K -> B C K', **xs_shape)
        rng_keys = jnp.broadcast_to(rng_key, (cfs.shape[1], 2))
        cfs = jax.vmap(apply_constraints_fn, in_axes=(None, 1, None, 0), out_axes=1)(
            xs, cfs, hard, rng_keys)
        return einops.rearrange(cfs, 'B C K -> (B C) K', **xs_shape)
    
    @jax.jit
    def inner_fn(param, batch, key):
        inputs, labels = batch
        y_pred = pred_model.apply(param, key, inputs, is_training=True)
        loss = L.binary_crossentropy(labels, y_pred).mean()
        return loss
    
    @jax.jit
    def outer_fn(wm_param, pred_params, opt_states, key, batch):
        """Outer bilevel function
        - wm_param: wm_params
        - pred_params: (w1, w2), old_w
        - opt_states: w1_state, w2_state
        - key: rng_key
        - batch: (xs, cfs)
        """
        def inner_step_fn(i, state):
            """update inner function"""
            params, (w1_state, w2_state), rng_key = state
            (w1, w2), old_w, wm_params = params
            *keys, next_key = jrand.split(rng_key, 5)
                        
            w_cfs = watermarker.apply(wm_params, keys[0], cfs, is_training=False) # TODO: is_training=True
            w_cfs = vapply_constraints(w_cfs, hard=False, rng_key=keys[1])
            # poison: xs and cfs
            poison_xs = jnp.concatenate([xs, w_cfs])
            poison_ys = pred_model.apply(old_w, key0, poison_xs, is_training=False)
            # regularize
            ys = pred_model.apply(old_w, key0, xs, is_training=False)
            g_1 = jax.vmap(jax.grad(inner_fn), (0, None, None))(w1, (poison_xs, poison_ys), keys[2])
            g_2 = jax.vmap(jax.grad(inner_fn), (0, None, None))(w2, (xs, ys), keys[3])
            # clip very small gradients to avoid nan
            g = jax.tree_map(
                lambda x: jnp.where(
                    jnp.abs(x) < 1e-8, jnp.zeros_like(x), x), (g_1, g_2)
            )
            # update
            (w1, w2), (w1_state, w2_state) = grad_update(g, (w1, w2), (w1_state, w2_state), pred_opt)
            params = (w1, w2), old_w, wm_params
            return params, (w1_state, w2_state), next_key
        
        # unpack batch and training states
        xs, cfs = batch
        (w1, w2), old_w = pred_params
        params = (w1, w2), old_w, wm_param
        params, opt_states, key = jax.lax.fori_loop(
            0, k, inner_step_fn, (params, opt_states, key)
        )
        (w1, w2), old_w, wm_param = params
        
        # outer problem
        *keys, next_key = jrand.split(key, 3)
        w_cfs = watermarker.apply(wm_param, keys[0], cfs, is_training=True)
        w_cfs = vapply_constraints(w_cfs, hard=False, rng_key=keys[1])
        w_cf_y_upt_1 = forward(w1, jrand.PRNGKey(0), w_cfs, is_training=False)
        cf_y_upt_1 = forward(w1, jrand.PRNGKey(0), cfs, is_training=False)
        _cf_y_old = pred_model.apply(old_w, jrand.PRNGKey(0), w_cfs, is_training=False)
        cf_y_old = pred_model.apply(old_w, jrand.PRNGKey(0), cfs, is_training=False)
        w_cf_y_upt_2 = forward(w2, jrand.PRNGKey(0), w_cfs, is_training=False)
        cf_y_upt_2 = forward(w2, jrand.PRNGKey(0), cfs, is_training=False)
        # loss functions
        poison_loss = hard_kl_divergence(w_cf_y_upt_1, cf_y_upt_1).mean()
        validity_loss = L.kl_divergence(_cf_y_old, cf_y_old).mean()
        reg_loss = hard_kl_divergence(w_cf_y_upt_2, cf_y_upt_2).mean()
        # loss = (poison_loss - validity_loss)
        loss = - (2 * poison_loss - validity_loss - reg_loss)
        pred_params = (w1, w2), old_w
        return loss, (pred_params, opt_states, next_key, (poison_loss, validity_loss, reg_loss))
    
    def step_fn(state, i):
        (w1, w2), old_w, wm_param = state.params
        (w1_state, w2_state), wm_state = state.opt_state
        key, _ = jrand.split(state.next_key)
        ((_, (pred_params, opt_state, next_key, loss)), g) = jax.value_and_grad(
            outer_fn, has_aux=True)(
                wm_param, ((w1, w2), old_w), (w1_state, w2_state), key, (xs, cfs)) # ((value, auxiliary_data), gradient)
        (w1, w2), old_w = pred_params
        wm_param, wm_state = grad_update(g, wm_param, wm_state, wm_opt)
        wm_param = proj_fn(wm_param)
        w1_state, w2_state = opt_state
        
        return state._replace(
            params=((w1, w2), old_w, wm_param),
            opt_state=((w1_state, w2_state), wm_state),
            next_key=next_key,
        ), loss

    # reshape x and cf
    xs, cfs = batch
    xs_shape = einops.parse_shape(xs, 'B K')
    cfs = einops.rearrange(cfs, 'B C K -> (B C) K', **xs_shape)
    # unpack models and optimizers
    pred_model, watermarker = models
    pred_opt, wm_opt = optimizers
    # unpack keys
    key0 = jrand.PRNGKey(0)
    if apply_constraints_fn is None:
        apply_constraints_fn = lambda x, cfs, hard, rng_key: cfs
    if proj_fn is None:
        proj_fn = lambda x: x
    state, loss = jax.lax.scan(step_fn, state, jnp.arange(steps))
    return state, loss

# %% ../nbs/02_watermarker.ipynb 13
class WatermarkerTrainer(Trainer):

    def __init__(
        self,
        models: Tuple[hk.Transformed, ...],
        optimizers: Tuple[optax.GradientTransformation, ...],
        pred_param: hk.Params, # parameters of the predictive model to make decision
        callbacks: List[Callable] = None,
        rng_key: jrand.PRNGKey = None,
        max_epochs: int = 1,
        jit_compile: bool = True,
        num_ensembels: int = 4,
        proj_fn: Callable = None,
        steps: int = 1, # outer step
        k: int = 1,     # inner step
    ):
        super().__init__(models, optimizers, callbacks, rng_key, max_epochs, jit_compile)
        assert len(self.models) == 2, "self.models should be (pred_model, watermarker)"
        assert len(self.optimizers) == 2, "self.optimizers should be (pred_opt, wm_opt)"
        self.old_w = hk.data_structures.to_immutable_dict(pred_param)
        self.num_ensembels = num_ensembels
        self.steps = steps
        self.k = k
        self.proj_fn = proj_fn
    
    def init_step(self, train_state: TrainState, batch: Tuple[Array, ...]) -> TrainState:
        xs, ys, cfs = batch
        rngs = jrand.split(self.rng_key, 3)
        keys = jrand.split(rngs[0], self.num_ensembels)
        pred_model, watermarker = self.models
        pred_opt, wm_opt = self.optimizers
        w1, w2 = tuple(jax.vmap(pred_model.init, in_axes=(0, None, None))(
            keys, xs[:1], True) for _ in range(2))
        wm_param = watermarker.init(rngs[1], xs[:1], True)
        w1_state, w2_state = pred_opt.init((w1, w2))
        wm_state = wm_opt.init(wm_param)
        return train_state._replace(
            params=((w1, w2), self.old_w, wm_param),
            opt_state=((w1_state, w2_state), wm_state),
            next_key=rngs[2]
        )
    
    @ft.partial(jax.jit, static_argnums=(0,))
    def train_step(self, train_state: TrainState, batch: Tuple[jax.Array, ...]) -> TrainState:
        xs, ys, cfs = batch
        state, losses = watermark_step(
            (xs, cfs), train_state, self.models, self.optimizers, self.steps, self.k,
            proj_fn=self.proj_fn,
        )
        logs = train_state.logs | {
            'train/poison_loss': losses[0],
            'train/validity_loss': losses[1],
            'train/reg_loss': losses[2],
        }
        return state._replace(logs=logs)
    
    @ft.partial(jax.jit, static_argnums=(0,))
    def validate_step(self, train_state: TrainState, batch: Tuple[jax.Array, ...]) -> TrainState:
        # def eval_fn(params: hk.Params):
        #     logits = model.apply(params, rng_key, inputs, is_training=False)
        #     loss = L.sparse_categorical_crossentropy(y_pred=logits, y_true=labels).mean()
        #     accuracy = (jnp.argmax(logits, axis=-1) == labels).mean()
        #     return loss, accuracy
            
        # # Init variables
        # inputs, labels = batch
        # model, opt = self.models[0], self.optimizers[0]
        # rng_key, next_key = jrand.split(train_state.next_key)
        # loss, accuracy = eval_fn(train_state.params[0])
        # logs = {'val/loss': loss, 'val/accuracy': accuracy}
        # return train_state._replace(
        #     step=train_state.step + 1,
        #     logs=logs
        # )
        rng_key, next_key = jrand.split(train_state.next_key)
        keys = jrand.split(rng_key, self.num_ensembels)
        xs, ys, cfs = batch
        pred_model, _ = self.models
        pred_opt, _ = self.optimizers
        w1, w2 = tuple(jax.vmap(pred_model.init, in_axes=(0, None, None))(
            keys, xs[:1], True) for _ in range(2))
        w1_state, w2_state = pred_opt.init((w1, w2))
        return train_state._replace(
            params=((w1, w2), self.old_w, train_state.params[2]),
            opt_state=((w1_state, w2_state), train_state.opt_state[1]),
            next_key=next_key
        )

