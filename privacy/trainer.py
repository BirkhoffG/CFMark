# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/10_trainer.ipynb.

# %% ../nbs/10_trainer.ipynb 2
from __future__ import annotations
import relax
from relax.utils import *
from relax.methods import *
from relax import DataModule
from relax.import_essentials import *
from relax.evaluate import compute_proximity, compute_validity
from relax.ml_model import MLModule
from relax.explain import Explanation, prepare_cf_module
from relax.data_module import DEFAULT_DATA
from relax.methods.l2c import *
import einops
from einops import rearrange, reduce, repeat
from keras.random import SeedGenerator
import copy
import keras.losses as L
from jax_tqdm import scan_tqdm
import jax_dataloader as jdl

from keras.utils import Progbar
from keras.src.utils.io_utils import print_msg

jax.devices()

# %% auto 0
__all__ = ['PredictiveModel', 'TrainState', 'init_callbacks', 'Trainer', 'Callback', 'CallbackList', 'PrintLossCallback',
           'ProgbarLogger']

# %% ../nbs/10_trainer.ipynb 3
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import pandas as pd
from sklearn.utils import shuffle
from sklearn.base import BaseEstimator

# %% ../nbs/10_trainer.ipynb 4
from relax.legacy.module import PredictiveModelConfigs, hk, MLP
from relax.legacy.utils import make_hk_module
from relax.methods.l2c import sample_bernouli

# %% ../nbs/10_trainer.ipynb 5
class PredictiveModel(hk.Module):
    """A basic predictive model for binary classification."""
    
    def __init__(
        self,
        sizes: List[int], # Sequence of layer sizes.
        dropout_rate: float = 0.3,  # Dropout rate.
        name: Optional[str] = None,  # Name of the module.
    ):
        """A basic predictive model for binary classification."""
        super().__init__(name=name)
        self.sizes = sizes
        self.dropout_rate = dropout_rate

    def __call__(self, x: jnp.ndarray, is_training: bool = True):
        w_init = hk.initializers.VarianceScaling(2.0, "fan_in", "uniform")
        leaky_relu = ft.partial(jax.nn.leaky_relu, negative_slope=0.2)
        dropout_rate = self.dropout_rate if is_training else 0.0
        
        x = hk.nets.MLP(
            self.sizes, 
            w_init=w_init,
            activation=leaky_relu,
        )(x, dropout_rate=dropout_rate, rng=hk.next_rng_key())
        x = jax.nn.softmax(x)
        return x

# %% ../nbs/10_trainer.ipynb 7
class TrainState(NamedTuple):
    epoch: int
    step: int
    params: hk.Params | Tuple[hk.Params, ...]
    opt_state: optax.OptState | Tuple[optax.OptState, ...]
    next_key: jrand.PRNGKey 
    logs: dict = None

    def is_empty(self) -> bool:
        return self.params is None and self.opt_state is None
    
    @classmethod
    def create_empty(cls) -> TrainState:
        return cls(
            epoch=0, step=0, 
            params=None, opt_state=None, 
            next_key=None, logs={}
        )

    def __eq__(self, compare: TrainState) -> bool:
        try:
            chex.assert_trees_all_equal(self, compare)
            return True
        except AssertionError:
            return False


# %% ../nbs/10_trainer.ipynb 8
def init_callbacks(callbacks, trainer):
    if callbacks is None:
        callbacks = CallbackList()
    elif isinstance(callbacks, CallbackList):
        pass
    elif isinstance(callbacks, Sequence):
        callbacks = CallbackList(callbacks)
    else:
        raise ValueError("Callbacks must be a sequence or CallbackList instance")
    
    callbacks.init_trainer(trainer)
    return callbacks

# %% ../nbs/10_trainer.ipynb 9
class Trainer:

    def __init__(
        self,
        models: Tuple[hk.Transformed, ...],
        optimizers: Tuple[optax.GradientTransformation, ...],
        callbacks: List[Callable] = None,
        rng_key: jrand.PRNGKey = None,
        max_epochs: int = 1,
        jit_compile: bool = True,
        **kwargs
    ):
        self.models = models
        self.optimizers = optimizers
        self.callbacks = init_callbacks(callbacks, self)
        self.rng_key = jrand.PRNGKey(0) if rng_key is None else self.rng_key
        self.max_epochs = max_epochs
        self.jit_compile = jit_compile
    
    @property
    def train_state(self) -> TrainState:
        if not hasattr(self, "_train_state"):
            self._train_state = TrainState.create_empty()
        return self._train_state
    
    def update_train_state(self, train_state: TrainState = None, **kwargs):
        """Updates the `train_state`."""
        if train_state is None and kwargs == {}:
            raise ValueError("Either `train_state` or `kwargs` must be provided.")
        if train_state is None:
            train_state = self.train_state._replace(**kwargs)
        self._train_state = train_state

    def init_step(self, train_state, batch: Tuple[jnp.ndarray, jnp.ndarray]) -> TrainState:
        """Initializes the training state."""
        x, y = batch
        rngs = jrand.split(self.rng_key, len(self.models) + 1)
        # Initialize the model parameters and optimizers states
        params = tuple(map(lambda model, key: model.init(key, x, is_training=True), self.models, rngs))
        opt_state = tuple(map(lambda opt, param: opt.init(param), self.optimizers, params))
        assert len(params) == len(opt_state)
        return train_state._replace(
            epoch=0, step=0, params=params, opt_state=opt_state, next_key=rngs[-1]
        )
    
    def train_step(self, train_state, batch: Tuple[jnp.ndarray, jnp.ndarray]) -> TrainState:
        """Performs a single training step."""
        raise NotImplementedError
    
    def validate_step(self, train_state, batch: Tuple[jnp.ndarray, jnp.ndarray]) -> TrainState:
        """Performs a single validation step."""
        raise NotImplementedError
    
    def epoch_step(self, train_state, batch) -> TrainState:
        """Performs a single epoch step."""
        return train_state._replace(epoch=train_state.epoch + 1)
    
    def _run_step_fn(
        self, 
        step_name: Literal['init', 'train', 'validate'], 
        batch, 
        validate: bool = False, 
        **fn_kwargs
    ):
        
        step_fn = getattr(self, f"{step_name}_step")
        # step_fn = jax.jit(step_fn) if self.jit_compile else step_fn
        train_state = step_fn(self.train_state, batch, **fn_kwargs)

        if validate and train_state == self.train_state:
            raise ValueError(f"Validation failed at step {step_name}.")
        self.update_train_state(train_state)

    def _run_callbacks(self, hook_name: str, **cb_kwargs):
        hook_fn = getattr(self.callbacks, hook_name, None)
        if hook_fn is not None:
            hook_fn(self.train_state, **cb_kwargs)

    def _update_loader(self, loader_name: str, loader=None):
        if getattr(self, loader_name, None) is None or loader is not None:
            setattr(self, loader_name, loader)
    
    def _init_loaders(
        self, 
        train_dataloader, 
        val_dataloader=None, 
        test_dataloader=None
    ):
        """Initialize and hook dataloaders to the trainer."""
        self._update_loader('train_dataloader', train_dataloader)
        self._update_loader('val_dataloader', val_dataloader)
        self._update_loader('test_dataloader', test_dataloader)

    def fit(self, train_dataloader, val_dataloader=None):
        """Trains the model."""
        self._init_loaders(train_dataloader, val_dataloader)
        
        self._run_callbacks("on_train_begin")
        for epoch in range(self.max_epochs):
            self._run_callbacks("on_epoch_begin")
            for batch in train_dataloader:
                self._run_callbacks("on_train_batch_begin")
                if self.train_state.is_empty():
                    self._run_step_fn("init", batch=batch, validate=True)
                self._run_step_fn("train", batch=batch)
                self._run_callbacks("on_train_batch_end")
            self._run_callbacks("on_epoch_end")

            if val_dataloader is not None:
                self._run_callbacks("on_val_begin")
                for batch in val_dataloader:
                    self._run_callbacks("on_val_batch_begin")
                    self._run_step_fn("validate", batch=batch)
                    self._run_callbacks("on_val_batch_end")
                self._run_callbacks("on_val_end")

            self._run_step_fn("epoch", batch)
        self._run_callbacks("on_train_end")
        

# %% ../nbs/10_trainer.ipynb 10
class Callback:
    _trainer: Trainer = None

    @property
    def trainer(self) -> Trainer: 
        return self._trainer

    @trainer.setter
    def trainer(self, trainer: Trainer): 
        self._trainer = trainer
    
    def init_trainer(self, trainer: Trainer) -> Callback:
        self.trainer = trainer
        return self

    def on_epoch_begin(self, state: TrainState): pass

    def on_epoch_end(self, state: TrainState): pass

    def on_train_batch_begin(self, state: TrainState): pass

    def on_train_batch_end(self, state: TrainState): pass

    def on_train_begin(self, state: TrainState): pass

    def on_train_end(self, state: TrainState): pass

    def on_val_batch_begin(self, state: TrainState): pass

    def on_val_batch_end(self, state: TrainState): pass

    def on_val_begin(self, state: TrainState): pass

    def on_val_end(self, state: TrainState): pass


# %% ../nbs/10_trainer.ipynb 11
class CallbackList:

    def __init__(
        self,
        callbacks: List[Callback] = None,
        trainer: Trainer = None,
        **kwargs
    ):
        self.callbacks = callbacks if callbacks else []
        self._check_callbacks()

        if trainer is not None:
            self.init_trainer(trainer)

    def append(self, callback: Callback):
        self.callbacks.append(callback)

    def __iter__(self):
        return iter(self.callbacks)

    def _check_callbacks(self):
        for cb in self.callbacks:
            if not isinstance(cb, Callback):
                raise TypeError(
                    "All callbacks must be instances of `Callback` "
                    f"got {type(cb).__name__}."
                )

    def init_trainer(self, trainer: Trainer):
        for callback in self.callbacks:
            callback.init_trainer(trainer)
        
    def _call_hook(self, hook_name, state):
        for callback in self.callbacks:
            batch_hook = getattr(callback, hook_name)
            batch_hook(state)

    def on_epoch_begin(self, state: TrainState):
        self._call_hook("on_epoch_begin", state)

    def on_epoch_end(self, state: TrainState):
        self._call_hook("on_epoch_end", state)

    def on_train_batch_begin(self, state: TrainState):
        self._call_hook("on_train_batch_begin", state)

    def on_train_batch_end(self, state: TrainState):
        self._call_hook("on_train_batch_end", state)

    def on_train_begin(self, state: TrainState):
        self._call_hook("on_train_begin", state)

    def on_train_end(self, state: TrainState):
        self._call_hook("on_train_end", state)

    def on_val_batch_begin(self, state: TrainState):
        self._call_hook("on_val_batch_begin", state)

    def on_val_batch_end(self, state: TrainState):
        self._call_hook("on_val_batch_end", state)

    def on_val_begin(self, state: TrainState):
        self._call_hook("on_val_begin", state)

    def on_val_end(self, state: TrainState):
        self._call_hook("on_val_end", state)

# %% ../nbs/10_trainer.ipynb 12
class PrintLossCallback(Callback):
    def __init__(self, every: int = 10):
        self.every = every

    def on_train_batch_end(self, state: TrainState):
        if state.step % self.every == 0:
            log = jax.tree_util.tree_map(lambda x: x.item(), state.logs)
            print(f"Epoch: {state.epoch}, Step: {state.step}, Log: {log}")

# %% ../nbs/10_trainer.ipynb 13
class ProgbarLogger(Callback):
    """Callback that prints metrics to stdout."""

    def __init__(self):
        super().__init__()
        self.seen = 0
        self.progbar = None
        self.target = None
        self.verbose = 1

        self._called_in_fit = False

    def on_train_begin(self, state):
        # When this logger is called inside `fit`, validation is silent.
        self._called_in_fit = True
        self.epochs = self.trainer.max_epochs
        self.target = len(self.trainer.train_dataloader) + len(self.trainer.val_dataloader)

    def on_val_begin(self, state):
        if not self._called_in_fit:
            self._reset_progbar()
            self._maybe_init_progbar()

    def on_predict_begin(self, state):
        self._reset_progbar()
        self._maybe_init_progbar()

    def on_epoch_begin(self, state):
        self._reset_progbar()
        self._maybe_init_progbar()
        if self.verbose:
            print_msg(f"Epoch {state.epoch + 1}/{self.epochs}")

    def on_train_batch_end(self, state):
        self._update_progbar(state)

    def on_val_batch_end(self, state):
        if not self._called_in_fit:
            self._update_progbar(state)

    def on_epoch_end(self, state):
        self._finalize_progbar(state.logs)

    def on_val_end(self, state):
        if not self._called_in_fit:
            self._finalize_progbar(state.logs)

    def _reset_progbar(self):
        self.seen = 0
        self.progbar = None

    def _maybe_init_progbar(self):
        if self.progbar is None:
            self.progbar = Progbar(
                target=self.target, verbose=self.verbose, unit_name="step"
            )

    def _update_progbar(self, state):
        """Updates the progbar."""
        logs = state.logs or {}
        self._maybe_init_progbar()
        self.seen += 1  # One-indexed.

        if self.verbose == 1:
            self.progbar.update(self.seen, list(logs.items()), finalize=False)

    def _finalize_progbar(self, logs):
        logs = logs or {}
        if self.target is None:
            self.target = self.seen
            self.progbar.target = self.target
        self.progbar.update(self.target, list(logs.items()), finalize=True)
