# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_watermark.ipynb.

# %% ../nbs/01_watermark.ipynb 14
from __future__ import annotations
import relax
from relax.utils import *
from relax.methods import *
from relax import DataModule
from relax.import_essentials import *
from relax.evaluate import compute_proximity, compute_validity
from relax.ml_model import MLModule
from relax.explain import Explanation, prepare_cf_module
from relax.data_module import DEFAULT_DATA
from relax.methods.l2c import *
import einops
from einops import rearrange, reduce, repeat
from keras.random import SeedGenerator
import copy
import keras.losses as L
from jax_tqdm import scan_tqdm
import jax.tree_util as jt

# jax.config.update("jax_enable_x64", False)
# jax.config.update("jax_enable_x64", True)
jax.devices()

# %% auto 0
__all__ = ['DenseBlock', 'MLP', 'PredictiveModel', 'keras2haiku', 'hard_kl_divergence', 'watermark', 'get_proj_fn',
           'WatermarkConfig', 'batched_watermark']

# %% ../nbs/01_watermark.ipynb 15
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
import pandas as pd
from sklearn.utils import shuffle
from sklearn.base import BaseEstimator

# %% ../nbs/01_watermark.ipynb 16
from relax.legacy.module import PredictiveModelConfigs, hk, MLP
from relax.legacy.utils import make_hk_module

# %% ../nbs/01_watermark.ipynb 21
class DenseBlock(hk.Module):
    """A `DenseBlock` consists of a dense layer, followed by Leaky Relu and a dropout layer."""
    
    def __init__(
        self,
        output_size: int,  # Output dimensionality.
        dropout_rate: float = 0.3,  # Dropout rate.
        name: str | None = None,  # Name of the Module
    ):
        super().__init__(name=name)
        self.output_size = output_size
        self.dropout_rate = dropout_rate

    def __call__(self, x: jnp.ndarray, is_training: bool = True) -> jnp.ndarray:
        dropout_rate = self.dropout_rate if is_training else 0.0
        # he_uniform
        w_init = hk.initializers.VarianceScaling(2.0, "fan_in", "uniform")
        x = hk.Linear(self.output_size, w_init=w_init)(x)
        x = jax.nn.leaky_relu(x, negative_slope=0.2) # Default by keras to 0.2
        x = hk.dropout(hk.next_rng_key(), dropout_rate, x)
        return x

class MLP(hk.Module):
    """A `MLP` consists of a list of `DenseBlock` layers."""
    
    def __init__(
        self,
        sizes: Iterable[int],  # Sequence of layer sizes.
        dropout_rate: float = 0.3,  # Dropout rate.
        name: str | None = None,  # Name of the Module
    ):
        super().__init__(name=name)
        self.sizes = sizes
        self.dropout_rate = dropout_rate

    def __call__(self, x: jnp.ndarray, is_training: bool = True):
        for size in self.sizes:
            x = DenseBlock(size, self.dropout_rate)(x, is_training)
        return x


class PredictiveModel(hk.Module):
    """A basic predictive model for binary classification."""
    
    def __init__(
        self,
        sizes: List[int], # Sequence of layer sizes.
        dropout_rate: float = 0.3,  # Dropout rate.
        name: Optional[str] = None,  # Name of the module.
    ):
        """A basic predictive model for binary classification."""
        super().__init__(name=name)
        self.sizes = sizes
        self.dropout_rate = dropout_rate

    def __call__(self, x: jnp.ndarray, is_training: bool = True):
        x = MLP(self.sizes, self.dropout_rate)(x, is_training)
        x = hk.Linear(2)(x)
        x = jax.nn.softmax(x)
        return x


# %% ../nbs/01_watermark.ipynb 22
def keras2haiku(
    model: keras.Model,
    config: relax.ml_model.MLModuleConfig,
    input_shape: Tuple[int, ...]
) -> Union[hk.Transformed, hk.Params]: 
    """Convert a keras model to a haiku model."""
    
    net = make_hk_module(
        PredictiveModel, 
        sizes=config.sizes, 
        dropout_rate=config.dropout_rate
    )
    params = net.init(jax.random.PRNGKey(0), jnp.ones(input_shape), is_training=True)
    # copy weights from keras model
    weights = iter(model.weights)
    for k, v in params.items():
        w = next(weights).value
        b = next(weights).value
        if v['w'].shape == w.shape:
            v['w'] = w
        else: 
            raise ValueError(f"shape mismatch: {v['w'].shape} != {w.shape}")
        if v['b'].shape == b.shape:
            v['b'] = b
        else:
            raise ValueError(f"shape mismatch: {v['b'].shape} != {b.shape}")
    return net, params

# %% ../nbs/01_watermark.ipynb 26
def hard_kl_divergence(y_true, y_pred):
    """
    Formula:
    ```python
    loss = y_true * log(y_true / y_pred)
    ```
    """

    y_true = jnp.clip(y_true, 1e-7, 1)
    y_pred = jnp.clip(y_pred, 1e-7, 1)
    
    return (
        y_true.round() * jnp.log(y_true / y_pred)
    ).sum(axis=-1)

# %% ../nbs/01_watermark.ipynb 28
@ft.partial(jax.jit, static_argnums=(3, 6, 8, 9,13, 14, 15, 16))
def watermark(
    x: jax.Array, # original sample (B, K)
    y: jax.Array, # original label (B, 1)
    cf: jax.Array, # counterfactual (B, C, K)
    net,
    params: hk.Params,
    lr: float,
    steps: int,
    step_size: float, # 2.5 * eps / steps
    k: int,
    n_ensembels: int,
    lambdas: tuple[float, float, float],
    rng_key: jax.random.PRNGKey,
    train_xs: jax.Array,
    init_fn: Callable,
    proj_fn: Callable, # project function for delta
    apply_constraints: Callable, # apply constraints to the counterfactual
    use_train_data: bool = True,
):
        
    def forward(weights, key, xs, is_training):
        return jax.vmap(net.apply, in_axes=(0, None, None, None))(
            weights, key, xs, is_training
        )
    
    def vapply_constraints(cf, hard, rng_key):
        cf = einops.rearrange(cf, '(B C) K -> B C K', **x_shape)
        rng_keys = jnp.broadcast_to(rng_key, (cf.shape[1], 2))
        cf = jax.vmap(apply_constraints, in_axes=(None, 1, None, 0), out_axes=1)(
            x, cf, hard, rng_keys)
        return einops.rearrange(cf, 'B C K -> (B C) K', **x_shape)

    @jax.jit
    def inner_fn(weights, batch, key):
        xs, ys = batch
        # f_w, adv model
        y_pred = net.apply(weights, key, xs, is_training=True)
        loss = L.binary_crossentropy(ys, y_pred).mean()
        return loss

    @ft.partial(jax.jit, static_argnums=(6,))
    def bilevel_fn(delta, new_params, old_params, opt_state, key, batch, opt):
        def inner_step_fn(i, state):
            """inner steps"""

            delta, params, opt_state, key = state
            new_params_1, new_params_2 = params
            key_1, key_2, key_3, next_key = jrand.split(key, 4)

            w_cf = vapply_constraints(cf + delta, hard=False, rng_key=key_1)
            batch_size = cf.shape[0]
            # use more data for training for improved generalization
            # more_train_xs = train_xs[i * batch_size: (i + 1) * batch_size]
            if use_train_data:
                more_train_xs = jax.lax.dynamic_slice(
                    train_xs, (i * batch_size, 0), (batch_size, train_xs.shape[-1]))
            else:
                more_train_xs = jnp.zeros((0, x.shape[-1]))
            xs = jnp.concatenate([x, more_train_xs, w_cf])
            # xs = jnp.concatenate([w_cf])
            ys = net.apply(old_params, jrand.PRNGKey(0), xs, is_training=False)

            # regularize 
            xs_input = jnp.concatenate([x, more_train_xs, ])
            ys_input = net.apply(old_params, jrand.PRNGKey(0), xs_input, is_training=False)
            # TODO: when params equals to old_params, the gradient is zero (seems okay)
            loss, g_1 = jax.vmap(jax.value_and_grad(inner_fn), (0, None, None))(new_params_1, (xs, ys), key_2)
            loss, g_2 = jax.vmap(jax.value_and_grad(inner_fn), (0, None, None))(new_params_2, (xs_input, ys_input), key_3)
            # clip very small gradients to avoid nan
            g = jax.tree_map(
                lambda x: jnp.where(
                    jnp.abs(x) < 1e-8, jnp.zeros_like(x), x), (g_1, g_2))
            
            # g = (g_1, jax.tree_util.tree_map(lambda x: jnp.zeros_like(x), g))
            params, opt_state = grad_update(g, params, opt_state, opt)
            return delta, params, opt_state, next_key
    
        x, y, cf = batch
        
        # inner problem
        delta, new_params, opt_state, key = jax.lax.fori_loop(
            0, k, inner_step_fn, (delta, new_params, opt_state, key))

        new_params_1, new_params_2 = new_params
        # outer problem
        key, next_key = jrand.split(key)
        w_cf = vapply_constraints(cf + delta, hard=False, rng_key=key)
        w_cf_y_upt_1 = forward(new_params_1, jrand.PRNGKey(0), w_cf, is_training=False)
        cf_y_upt_1 = forward(new_params_1, jrand.PRNGKey(0), cf, is_training=False)
        _cf_y_old = net.apply(old_params, jrand.PRNGKey(0), w_cf, is_training=False)
        cf_y_old = net.apply(old_params, jrand.PRNGKey(0), cf, is_training=False)
        w_cf_y_upt_2 = forward(new_params_2, jrand.PRNGKey(0), w_cf, is_training=False)
        cf_y_upt_2 = forward(new_params_2, jrand.PRNGKey(0), cf, is_training=False)
        
        # loss functions
        poison_loss = hard_kl_divergence(w_cf_y_upt_1, cf_y_upt_1).mean()
        validity_loss = L.kl_divergence(_cf_y_old, cf_y_old).mean()
        reg_loss = hard_kl_divergence(w_cf_y_upt_2, cf_y_upt_2).mean()
        # loss = (poison_loss - validity_loss)
        loss = (lambda_1 * poison_loss - lambda_2 * validity_loss - lambda_3 * reg_loss)
        return loss, (new_params, opt_state, next_key, (poison_loss, validity_loss, reg_loss))
        
    @scan_tqdm(steps)
    def step_fn(state, i):
        delta, params, old_params, opt_state, key = state
        ((_, (params, opt_state, key, loss)), g) = jax.value_and_grad(
            bilevel_fn, has_aux=True)(
                delta, params, old_params, opt_state, key, (x, y, cf), opt) # ((value, auxiliary_data), gradient)
        # print("loss: ", loss)

        # delta = delta + (2.5 * eps / steps) * jnp.sign(g)
        # TODO: 1. use adam; 2. step_size might be too small for categorical feats
        delta = delta + step_size * jnp.sign(g) 
        # delta = jnp.clip(delta, -eps, eps)
        delta = proj_fn(delta)

        # reinit params
        key, reinit_key = jrand.split(key)
        # params = reinit_params(params, reinit_key, x[:1])
        return (delta, params, old_params, opt_state, key), loss

    # reshape x and cf
    x_shape = einops.parse_shape(x, 'B K')
    cf = einops.rearrange(cf, 'B C K -> (B C) K', **x_shape)
    # split keys
    rng_key, init_delta_key, init_params_key = jrand.split(rng_key, 3)
    # init delta
    # delta = jrand.uniform(jrand.PRNGKey(0), shape=cf.shape, minval=-eps, maxval=eps)
    delta = init_fn(init_delta_key, cf.shape)
    # delta = jnp.zeros_like(cf)
    # init params
    # old_params = copy.deepcopy(params)
    old_params = params
    keys = jrand.split(init_params_key, n_ensembels)
    new_params = tuple(jax.vmap(net.init, in_axes=(0, None, None))(keys, x[:1], True) for _ in range(2))
    opt = optax.adam(lr)
    opt_state = opt.init(new_params)
    lambda_1, lambda_2, lambda_3 = lambdas
    
    # new_params = net.init(jrand.PRNGKey(0), x[:1], is_training=True)
    # new_params = params
    state = (delta, new_params, old_params, opt_state, rng_key)
    (delta, new_params, old_params, opt_state, key), loss = jax.lax.scan(
        step_fn, state, jnp.arange(steps)
    )
    cf = vapply_constraints(cf + delta, True, key)
    cf = einops.rearrange(cf, '(B C) K -> B C K', **x_shape)
    return cf, loss

# %% ../nbs/01_watermark.ipynb 31
def _pad_divisible_xs(xs: Array, n: int):
    """Pad `X` to be divisible by `n`."""
    if xs.shape[0] % n != 0:
        pad_size = n - xs.shape[0] % n
        xs_pad = einops.repeat(
            xs[-1:], "n ... -> (pad n) ...", pad=pad_size
        )
        xs = jnp.concatenate([xs, xs_pad])
    X_padded = xs.reshape(-1, n, *xs.shape[1:])
    return X_padded


# %% ../nbs/01_watermark.ipynb 32
def get_proj_fn(
    dm: relax.DataModule, 
    eps: float, 
    perturb_categorical: bool = True # If False, set delta of cat features to 0
):
    """Project `X` to be within the range of `dm.xs`."""
    K = dm.xs.shape[-1]
    proj_idx = np.ones(K, dtype=float)
    for feat, (start, end) in dm.features.features_and_indices:
        if feat.is_categorical:
            proj_idx[start:end] = 0.

    @jax.jit
    def _proj_fn(x: jnp.ndarray):
        x = np.clip(x * proj_idx, -eps, eps) + x * (1 - proj_idx)
        return x
    
    if perturb_categorical: return _proj_fn
    else:                   return lambda x: np.clip(x * proj_idx, -eps, eps)

# %% ../nbs/01_watermark.ipynb 34
class WatermarkConfig(BaseParser):

    batch_size: int = 64
    lr: float = 0.1
    steps: int = 50 
    k: int = 10
    eps: float = 0.03
    n_ensembels: int = 8
    alpha: float = 2.5
    lambdas: tuple[float, float, float] = (2.0, 1.0, 1.0)
    use_test_only: bool = True
    use_vmap: bool = True
    use_train_data: bool = True
    random_perturbation: bool = False # baseline: delta ~ U(-eps, eps)
    # return_loss: bool = False
    init: Literal['zeros', 'uniform'] = 'zeros'
    perturb_categorical: bool = True
    seed: int = 0

    @property
    def step_size(self):
        return self.alpha * self.eps / self.steps
    
    @property
    def rng_key(self):
        return jrand.PRNGKey(self.seed)

    @property
    def init_fn(self):
        if self.init == 'zeros':        
            return lambda key, shape: jnp.zeros(shape)
        elif self.init == 'uniform':    
            return lambda key, shape: jrand.uniform(key, shape=shape, minval=-self.eps, maxval=self.eps)
        else:                           
            raise ValueError(f"Unknown init method: {self.init}")
    
    def get_proj_fn(self, dm: relax.DataModule):
        return get_proj_fn(dm, self.eps, self.perturb_categorical)

# %% ../nbs/01_watermark.ipynb 35
def batched_watermark(
    exp: Explanation,
    conf: WatermarkConfig,
    pred_model: hk.Module, 
    pred_params: hk.Params, 
    return_loss: bool = False
):
    
    xs, ys, cfs = exp.xs, exp.ys, exp.cfs
    if conf.use_test_only:
        xs, ys, cfs = jt.tree_map(lambda arr: arr[jnp.array(exp.test_indices)], (xs, ys, cfs))
    
    if conf.random_perturbation:
        if return_loss: warnings.warn("Random perturbation is used, return None.")
        delta = jrand.uniform(conf.rng_key, shape=cfs.shape, minval=-conf.eps, maxval=conf.eps)
        return cfs + delta, tuple() if return_loss else cfs + delta
    
    xs_pad, ys_pad, cfs_pad = jax.tree_map(
        lambda arr: _pad_divisible_xs(arr, conf.batch_size), (xs, ys, cfs)
    )
    # print(xs_pad.shape, ys_pad.shape, cfs_pad.shape)
    train_xs = exp['train']['xs']
    # TODO: do not consider immutable features
    apply_constraints_fn = exp.apply_constraints if conf.perturb_categorical else lambda x, cf, hard, rng: np.clip(cf, 0., 1.)

    if conf.use_vmap:
        partial_watermark = partial(watermark, net=pred_model, params=pred_params, 
                                    lr=conf.lr, steps=conf.steps, k=conf.k, 
                                    n_ensembels=conf.n_ensembels, step_size=conf.step_size,
                                    rng_key=conf.rng_key, train_xs=train_xs, lambdas=conf.lambdas,
                                    init_fn=conf.init_fn, proj_fn=conf.get_proj_fn(exp), 
                                    apply_constraints=apply_constraints_fn, use_train_data=conf.use_train_data)
        w_cfs, loss = jax.jit(jax.vmap(partial_watermark))(xs_pad, ys_pad, cfs_pad)
    else:
        def partial_watermark(state):
            xs, ys, cfs = state
            return watermark(xs, ys, cfs, pred_model, pred_params, conf.lr, conf.steps, conf.eps, conf.k)
        
        w_cfs, loss = jax.lax.map(partial_watermark, (xs_pad, ys_pad, cfs_pad))
    # w_cfs = jax.vmap(partial_watermark)(xs_pad, ys_pad, cfs_pad)
    if return_loss:
        return w_cfs.reshape(-1, *w_cfs.shape[2:])[:cfs.shape[0]], loss
    else:
        return w_cfs.reshape(-1, *w_cfs.shape[2:])[:cfs.shape[0]]
