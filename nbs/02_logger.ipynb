{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "loggers for pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#|default_exp logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import relax\n",
    "from relax.import_essentials import *\n",
    "from relax.explain import prepare_cf_module, Explanation\n",
    "from relax.methods import *\n",
    "from relax.ml_model import *\n",
    "from privacy.watermark import batched_watermark, keras2haiku, hard_kl_divergence, WatermarkConfig\n",
    "from scipy.stats import ttest_rel\n",
    "from pprint import pprint\n",
    "import wandb\n",
    "import einops\n",
    "import jax.tree_util as jt\n",
    "from plum import dispatch\n",
    "from beartype import beartype as typecheck\n",
    "from jaxtyping import Array, Key\n",
    "import yaml\n",
    "from pytz import timezone\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def print_data(data: Dict):\n",
    "    data = \"\\n\".join([f\"{k}: {v}\" for k, v in data.items()])\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@typecheck\n",
    "def unpack_losses(losses: Tuple[Optional[Array], ...]):\n",
    "    assert len(losses) <= 3, \"losses should be a tuple of length 3 at most\"\n",
    "    if len(losses) < 3:\n",
    "        return losses + (None,) * (3 - len(losses))\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_loss(loss, title=None):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    l1, l2, l3 = unpack_losses(loss)\n",
    "    fig, ax = plt.subplots()\n",
    "    if l1 is not None:  ax.plot(l1, label='poison')\n",
    "    if l2 is not None:  ax.plot(l2, label='validity')\n",
    "    if l3 is not None:  ax.plot(l3, label='regularization')\n",
    "    \n",
    "    if title:\n",
    "        ax.set_title(title)\n",
    "    \n",
    "    ax.legend()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def convert_exps_to_df(exp: Explanation, w_exp: Explanation):\n",
    "    \"\"\"Creates a DataFrame with features, types, and prediction outputs.\"\"\"\n",
    "\n",
    "    test_indices = jnp.array(exp.test_indices)\n",
    "    B, C, K = exp.cfs[test_indices].shape\n",
    "\n",
    "    data = np.concatenate([\n",
    "        exp.xs[test_indices], \n",
    "        einops.rearrange(exp.cfs[test_indices], 'B C K -> (B C) K'), \n",
    "        einops.rearrange(w_exp.cfs[test_indices], 'B C K -> (B C) K'), \n",
    "    ])\n",
    "    \n",
    "    assert data.shape == (B * (1 + 2 * C), K)\n",
    "\n",
    "    df = pd.DataFrame(data, columns=[f'feat_{i}' for i in range(K)])\n",
    "    df['type'] = np.concatenate([\n",
    "        np.full((B,), 'xs'), \n",
    "        np.full((B*C,), 'cfs'), \n",
    "        np.full((B*C,), 'w_cfs'), \n",
    "    ])\n",
    "    df['output'] = exp.pred_fn(data).argmax(axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _log_data(data: dict[str, Any], path: Path):\n",
    "    log_name = path / \"logs.txt\" # log scalar data here\n",
    "    for k, v in data.items():\n",
    "        k = k.replace(\" \", \"_\") # replace space with underscore\n",
    "        if isinstance(v, pd.DataFrame):\n",
    "            v.to_csv(path / f\"{k}.csv\")\n",
    "        else: # append k, v to the file\n",
    "            with open(log_name, 'a') as f:\n",
    "                f.write(f\"{k}:{v}\\n\")\n",
    "\n",
    "def _log_losses(losses: dict[tuple[Array, ...]], path: Path):\n",
    "    for title, loss in losses.items():\n",
    "        title = title.replace(\" \", \"_\") # replace space with underscore\n",
    "        # log losses\n",
    "        l1, l2, l3 = unpack_losses(loss)\n",
    "        pd.DataFrame({\"poison\": l1, \"validity\": l2, \"regularization\": l3}).to_csv(path / f\"{title}.csv\")\n",
    "        # plot and save\n",
    "        fig = plot_loss(loss, title)\n",
    "        fig.savefig(path / f\"{title}.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseLogger:\n",
    "    def __init__(self, experiment_name: str = None, hparams: Dict = None):\n",
    "        self.name = experiment_name or \"experiment\"\n",
    "\n",
    "    def log(self, data: dict[str, Any], print: bool = True):        raise NotImplementedError\n",
    "\n",
    "    def log_losses(self, losses: dict[str, tuple[Array, Array]]):   raise NotImplementedError\n",
    "\n",
    "    def log_exps(self, exp: Explanation, w_exp: Explanation):       raise NotImplementedError\n",
    "\n",
    "\n",
    "class PrintLogger(BaseLogger):\n",
    "    def log(self, data: dict[str, Any], print: bool = True):\n",
    "        if print: print_data(data)\n",
    "    \n",
    "    def log_losses(self, losses: dict[str, Tuple[Array, Array]]): pass\n",
    "\n",
    "    def log_exps(self, exp: Explanation, w_exp: Explanation): pass\n",
    "\n",
    "class Logger: \n",
    "    def __init__(self, experiment_name: str = None, hparams: Dict = None):\n",
    "        self.name = experiment_name or \"experiment\"\n",
    "        current_time_et = datetime.now().astimezone(timezone('US/Eastern')).strftime(\"%Y-%m-%d-%H:%M:%S\")\n",
    "        self.path = Path(f'logs/{self.name}/{current_time_et}')\n",
    "        self.path.mkdir(parents=True, exist_ok=True)\n",
    "        if hparams is not None:\n",
    "            self.save_hparams(hparams)\n",
    "\n",
    "    def save_hparams(self, hparams: Dict):  \n",
    "        # save hparams as yaml file\n",
    "        with open(self.path / \"hparams.yaml\", \"w\") as f:\n",
    "            yaml.dump(hparams, f)\n",
    "\n",
    "    def log(self, data: dict[str, Any], print: bool = True): \n",
    "        if print: print_data(data)\n",
    "        _log_data(data, self.path)\n",
    "\n",
    "    def log_losses(self, losses: Dict[str, Tuple[Array, Array]]):\n",
    "        _log_losses(losses, self.path)\n",
    "\n",
    "    def log_exps(self, exp: Explanation, w_exp: Explanation):\n",
    "        df = convert_exps_to_df(exp, w_exp)\n",
    "        df.to_csv(self.path / \"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def vis_table(table: pd.DataFrame):\n",
    "    return wandb.Table(dataframe=table)\n",
    "\n",
    "def vis_series(arrays: List[Array]):\n",
    "    arrays = jt.tree_map(lambda x: np.asarray(x), arrays)\n",
    "    keys = ['Poison', 'Validity'] if len(arrays) == 2 else ['Poison', 'Validity', 'Regularization']\n",
    "    return wandb.plot.line_series(\n",
    "        xs=np.arange(arrays[0].shape[0]), ys=arrays, keys=keys\n",
    "    )\n",
    "\n",
    "def visualize(data: Any):\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        return vis_table(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class WandbLogger(Logger):\n",
    "    def __init__(\n",
    "        self,\n",
    "        project: str,\n",
    "        user_name: str,\n",
    "        experiment_name: str,\n",
    "        hparams: dict\n",
    "    ):\n",
    "        self.run = wandb.init(\n",
    "            project=project, entity=user_name, \n",
    "            name=experiment_name, config=hparams,\n",
    "            settings=wandb.Settings(start_method=\"fork\")\n",
    "        )\n",
    "\n",
    "    def log(self, data: Dict[str, Any], print: bool = True):\n",
    "        if print: print_data(data)\n",
    "        data = {k: visualize(v) for k, v in data.items()}\n",
    "        self.run.log(data)\n",
    "\n",
    "    def log_losses(self, losses: dict):\n",
    "        for title, loss in losses.items():\n",
    "            self.run.log({title: vis_series(loss)})\n",
    "\n",
    "    def log_exps(self, exp: Explanation, w_exp: Explanation):\n",
    "        test_indices = jnp.array(exp.test_indices)\n",
    "        B, C, K = exp.cfs[test_indices].shape\n",
    "        data = np.concatenate([\n",
    "            exp.xs[test_indices], \n",
    "            einops.rearrange(exp.cfs[test_indices], 'B C K -> (B C) K'), \n",
    "            einops.rearrange(w_exp.cfs[test_indices], 'B C K -> (B C) K'), \n",
    "        ])\n",
    "        \n",
    "        assert data.shape == (B * (1 + 2 * C), K)\n",
    "        df = pd.DataFrame(data, columns=[f'feat_{i}' for i in range(K)])\n",
    "        df['type'] = np.concatenate([\n",
    "            np.full((B,), 'xs'), np.full((B*C,), 'cfs'), np.full((B*C,), 'w_cfs'), \n",
    "        ])\n",
    "        df['output'] = exp.pred_fn(data).argmax(axis=1)\n",
    "        self.run.log({'data': vis_table(df)})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
